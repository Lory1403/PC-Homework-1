% arara: pdflatex: { draft: yes, shell: yes, options: [--output-directory=build] }

%! arara: makeglossaries: { options: [ '-d', 'build' ] }
%! arara: --> if changed (toFile('build/report.glo')) || missing (toFile('build/report.gls'))

% arara: pdflatex: { shell: yes, synctex: yes, options: [--output-directory=build] }

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{settings/set-color}
\usepackage{settings/set-geometry}
\usepackage{settings/set-lists}
\usepackage{settings/set-graphics}
\usepackage{settings/set-tables}
\usepackage{settings/set-spacing}
\usepackage{settings/set-code}
\usepackage{settings/set-plots}

\begin{document}

\title{Introduction to Parallel Computing\\
    Homework 1: Implicit parallelism techniques and performance models.\\
    \textbf{Results report}
}
\author{Lorenzo Fasol, 18244, lorenzo.fasol@studenti.unitn.it}
\date{October 2023}
\maketitle

\section{Array addition and vectorization}
In the first section of the Homework, both \textit{Task1} and \textit{Task2} involve the element-wise addition of two arrays, defined as A and B. This operation is executed through the implementation of two distinct routines, denoted as \texttt{routine1()} and \texttt{routine1()}, respectively. The result of this addition is stored in another array, denoted as C, of the same size as arrays A and B.\\
%inserisci che le due rotuine sono uguali quindi la differenza Ã¨ in compilazione
Given the necessity for Task 2 to incorporate implicit parallelism techniques, it is crucial to highlight that the actual code implementation for both tasks remains constant. While the code structure remains unaltered, the essential distinction emerges in how the compiler handles and optimizes the code.\\
\textit{Task1} can be seamlessly compiled to yield a sequential program, with no additional requirements. In contrast, \textit{Task2} demands the incorporation of specific compilation flags designed to harness implicit parallelism techniques. These flags are essential in the compilation process, enabling the code to exploit parallelism and optimize performance.\\
The compiler used, \texttt{GNU C++ compiler (G++)} in this specific case, offers a range of flag combinations for optimizing code. Following extensive experimentation to determine the most effective combination of these flags that leads to an optimal result, I have arrived at the following conclusions. Two potential solutions have emerged, yet I favour the second option due to its higher level of specificity and detail.\\\\
The solutions are the following:
\begin{itemize}
  \item \texttt{g++ -O3 hw1\_pt1.cpp -o hw1\_pt1.out}\\
  This only flag enables the utilization of a set of optimizations like, for example, function in-lining and instruction scheduling that are not specific for parallelism but can be beneficial to achieve better performance.
  \item \texttt{g++ -O2 -ftree-vectorize -funroll-loops -fprefetch-loop-arrays -march=native hw1\_pt1.cpp -o hw1\_pt1.out}\\\\
  Flags explanation:
    \begin{itemize}
        \item[] \textbf{-O2}: applies a range of sophisticated optimization techniques aiming to reduce the runtime execution time of the compiled program, thereby enhancing its efficiency and speed. 
        This optimization level strikes a balance, providing improved program efficiency without delving into more aggressive optimization levels, such as \texttt{-O3}.
        \item[] \textbf{-ftree-vectorize}: enable tree vectorization, which empowers the compiler to intelligently reconfigure loops. This optimization significantly enhances parallelism by enabling the simultaneous processing of multiple data elements within a single instruction.
        \item[] \textbf{-funroll-loops}: instructs the compiler to unroll loops. The compiler generates several duplicated instances of the loop body to mitigare the overhead associated with loops and to improve instruction-level parallelism.
        \item[] \textbf{-fprefetch-loop-arrays}: prompt the compiler to incorporate prefetching instructions within the generated code. Prefetching is a technique that aims to pre-load data into the cache before its actual demand, effectively mitigating data access latencies and enhancing parallelism.
        \item[] \textbf{-march=native}: used to optimize code for the specific microarchitecture of the host machine. Empowers the compiler to generate instructions that are finely tuned for the processor executing the code, maximizing performance. However, it ties the compiled code to the host machine's architecture.
    \end{itemize}
\end{itemize}
To conduct the tests, the default version of the program was employed, where three tests are performed for each array size within the range of $2^4$ to $2^{22}$. However, the C++ script can be executed with a user-defined array size by using a specific flag along with the corresponding argument. The test results have been stored in a \texttt{.CSV} file for the subsequent analysis. 

\subsection*{Results analysis}
I opted to perform the tests on my laptop instead of the cluster due to the lack of support for the required libraries in the cluster's G++ version.
To provide a criteria for comparison, I provide the technical features of the machine used for the tests:
\begin{itemize}
    \item \textbf{CPU}: Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-9750H at 2.60GHz
    \item \textbf{RAM}: 16GB
    \item \textbf{OS}: Windows 11 Pro
    \item \textbf{WSL}: Ubuntu 20.04.2 LTS
    \item \textbf{G++ compiler}: 9.4.0
\end{itemize}



The execution times of the sequential and parallel versions of the program are presented in the following table and graph.
In addition to the average execution times, the speedup and percentage gain values are also provided, which are calculated as follows:
\begin{equation}
    Speedup(S) = \frac{Sequential Execution Time}{Parallel Execution Time}
\end{equation} \\
\begin{equation}
    Percentage Gain(PG) = \frac{Sequential Execution Time - Parallel Execution Time}{Sequential Execution Time * 100}
\end{equation}

\begin{figure}[h!tb]
    \centering
    \captionsetup{
        type=plot,
        belowskip=10pt,
    }
    \caption{Loop execution times by array size}
    \begin{tikzpicture}
        \begin{axis}[
            width=0.9\textwidth,
            height=0.6\textwidth,
            xlabel={Array Size (2\symbol{"005E}Exp)},
            ylabel={Time (milliseconds)},
            legend pos=north west,
            grid=major,
        ]

        \addplot table [x=Exp, y=Average] {
            Exp Average
            4	0.001
            5	0.001
            6	0.001
            7	0.001
            8	0.001
            9	0.002
            10	0.003
            11	0.005
            12	0.010
            13	0.020
            14	0.042
            15	0.081
            16	0.163
            17	0.326
            18	0.660
            19	1.314
            20	2.545
            21	5.064
            22	10.471
        };

        \addplot table [x=Exp, y=Average] {
            Exp Average
            4	0.001
            5	0.001
            6	0.001
            7	0.001
            8	0.001
            9	0.001
            10	0.001
            11	0.002
            12	0.002
            13	0.005
            14	0.008
            15	0.018
            16	0.035
            17	0.072
            18	0.150
            19	0.333
            20	0.549
            21	1.143
            22	2.919
        };
        \legend{Sequential, Parallel}

        \end{axis}
    \end{tikzpicture}
\end{figure}

\clearpage

\begin{table}[h!tb]
\centering
\caption{Average execution times by array size (times in milliseconds)}
\begin{tabular}{@{} c c c c c @{}}
\toprule
    \textbf{ArraySize} & \textbf{AvgSeqExecTime} & \textbf{AvgParExecTime} & \textbf{Speedup} & \textbf{Percentage Gain}\\
\midrule
    $2^4$ & 0.001 & 0.001 & 1.00 & 0.00\%\\
\lightrule
    $2^5$ & 0.001 & 0.001 & 1.00 & 0.00\%\\
\lightrule
    $2^6$ & 0.001 & 0.001 & 1.00 & 0.00\%\\
\lightrule
    $2^7$ & 0.001 & 0.001 & 1.00 & 0.00\%\\
\lightrule
    $2^8$ & 0.001 & 0.001 & 1.00 & 0.00\%\\
\lightrule
    $2^9$ & 0.002 & 0.001 & 2.00 & 50.00\%\\
\lightrule
    $2^{10}$ & 0.003 & 0.001 & 3.00 & 66.67\%\\
\lightrule
    $2^{11}$ & 0.005 & 0.002 & 2.50 & 60.00\%\\
\lightrule
    $2^{12}$ & 0.010 & 0.002 & 5.00 & 80.00\%\\
\lightrule
    $2^{13}$ & 0.020 & 0.005 & 4.00 & 75.00\%\\
\lightrule
    $2^{14}$ & 0.042 & 0.008 & 5.25 & 81.82\%\\
\lightrule
    $2^{15}$ & 0.081 & 0.018 & 4.50 & 77.78\%\\
\lightrule
    $2^{16}$ & 0.163 & 0.035 & 4.66 & 78.53\%\\
\lightrule
    $2^{17}$ & 0.326 & 0.072 & 4.53 & 77.91\%\\
\lightrule
    $2^{18}$ & 0.660 & 0.150 & 4.40 & 77.27\%\\
\lightrule
    $2^{19}$ & 1.314 & 0.333 & 3.95 & 74.66\%\\
\lightrule
    $2^{20}$ & 2.545 & 0.549 & 4.64 & 78.43\%\\
\lightrule
    $2^{21}$ & 5.064 & 1.143 & 4.44 & 77.46\%\\
\lightrule
    $2^{22}$ & 10.471 & 2.919 & 3.59 & 72.15\%\\
\midrule
    \textbf{Average} & & & \textbf{3.23} & \textbf{54.09\%}\\
\bottomrule
\end{tabular}
\end{table}

It is evident that, as the array size increases, the execution times also increase for both sequential and parallel implementations. %
This trend becomes more pronounced when the array size exceeds the dimension of $2^{15}$, equivalent to 32,768 elements. \\
This behavior aligns with our expectations, as larger arrays inherently require more processing time due to the increased volume of data %
to be processed. Consequently, the trend reinforces the relationship between array size and execution time, providing valuable insights into how computational demands scale with data size.

\clearpage
\section{Matrix copy via block reverse ordering}
In the second section of the homework, like the preceding section, both \textit{Task1} and \textit{Task2} entail the execution of identical actions. %
In this instance, the primary emphasis remains directed toward the compilation flags employed during the compilation process. The compilation commands and their associated flags remain unchanged from those utilized in previous instances but applied to the code files specific to this part.\\
In this scenario, the program operates on a matrix, defined as M, with a size denoted as n, which is simplified for clarity to be a power of two. %
The matrix is divided into blocks, each with a size of b.\\
The primary task involves performing a specific action: copying each block, along with its associated content, in the reverse order, while assuming a row-major ordering (with the internal order within the block remaining unchanged). The outcome of this operation is stored in an output matrix, referred to as O, which shares the same size as matrix M.
The following code is designed to perform the task operating with matrices of power-of-two dimensions.

\begin{code}
    \captionof{listing}{\label{code:matrix}Swapping block algorithm}
    \begin{minted}{cpp}
for (int i = 0; i < dim / 2; i += b) {
    for (int j = 0; j < dim; j += b) {
        for (int k = 0; k < b; k++) {
            for (int l = 0; l < b; l++) {
                o[i + k][j + l] = m[dim - i - (b - k)][dim - j - (b - l)];
                o[dim - i - (b - k)][dim - j - (b - l)] = m[i + k][j + l];
            }
        }
    }
}
    \end{minted}
\end{code}

To conduct the tests, the n size was assumed to be equal to $2^{12}$. As before have been performed three tests for each size of the block within the range of $2^2$ to $2^8$.
\subsection*{Results analysis}
The results are obtained with the same machine and compiler used in the previous section to ensure data consistency. %
Similar to the preceding analysis, the average execution time for the three tests has been calculated, as well as the speedup and %
percentage gain values.

\begin{figure}[h!tb]
    \centering
    \captionsetup{
        type=plot,
        belowskip=10pt,
    }
    \caption{Loop execution time by block size}
    \begin{tikzpicture}
        \begin{axis}[
            width=0.8\textwidth,
            height=0.5\textwidth,
            xlabel={Matrix Size (2\symbol{"005E}Exp)},
            ylabel={Time (milliseconds)},
            legend style={
                at={(0.03,0.5)}, % Adjust the (x, y) coordinates for the desired position
                anchor=west,
            },
            grid=major,
        ]

        \addplot table [x=Exp, y=Average] {
            Exp Average
            2	133.206
            3	136.908
            4	143.39
            5	138.891
            6	128.640
            7	140.778
            8	127.564
        };

        \addplot table [x=Exp, y=Average] {
            Exp Average
            2	22.545
            3	17.453
            4	19.211
            5	14.544
            6	12.629
            7	12.700
            8	12.195
        };
        \legend{Sequential, Parallel}

        \end{axis}
    \end{tikzpicture}
\end{figure}

As evidenced by the provided data and relative graph, this scenario permits a better understanding of the impact of compiler flags on program performance.\\
The parallel version of the program consistently outperforms the sequential version, with the execution time of the parallel version being significantly lower than that of the sequential version.\\
In the graph the difference is striking, with the parallel version's execution time being approximately one-tenth of the sequential version's execution time. %
The speedup and percentage gain values are also significantly higher than those obtained in the previous section.


\begin{table}[h!tb]
    \centering
    \caption{Average execution times by block size (times in milliseconds)}
    \begin{tabular}{@{} c c c c c @{}}
    \toprule
        \textbf{BlockSize} & \textbf{AvgSeqExecTime} & \textbf{AvgParExecTime} & \textbf{Speedup} & \textbf{Percentage Gain}\\
        \midrule
        $2^2$ & 133.206 & 22.545 & 5.91 & 83.06\%\\
    \lightrule
        $2^3$ & 136.908 & 17.453 & 7.85 & 87.27\%\\
    \lightrule
        $2^4$ & 143.390 & 19.211 & 7.46 & 86.60\%\\
    \lightrule
        $2^5$ & 138.891 & 14.544 & 9.56 & 89.51\%\\
    \lightrule
        $2^6$ & 128.640 & 12.629 & 10.20 & 90.18\%\\
    \lightrule
        $2^7$ & 140.778 & 12.700 & 11.10 & 91.00\%\\
    \lightrule
        $2^8$ & 127.564 & 12.195 & 10.47 & 90.44\%\\
    \midrule
        \textbf{Average} & & & \textbf{8.93} & \textbf{88.29\%}\\
    \bottomrule
    \end{tabular}
    \end{table}

The effective bandwidth, which is indicative of the memory operation cost, relies on three values for its computation. %
The first two, signifying the data flow in terms of read and write bytes, are denoted as $B_r$ and $B_w$ respectively. %
Both of these values are identical and are calculated by multiplying the number of elements in the matrix ($n^2$ elements) by the byte size of the data type (in this specific case is a float, $S_f$, so 4 bytes).
This property stems from the algorithm's characteristic of processing each matrix element only once, resulting in only one read and one write operation for each element of the matrix.\\
The third value, denoted as t, represents the time required for the routine's execution, quantifying the duration in seconds. So the effective bandwidth can be calculated as follows:
\begin{equation}
    B_r = B_w = B_{r/w} = S_f \cdot n^2 = 4 \cdot (2^{12})^2 = 4 \cdot 4096^2\qquad[\textnormal{bytes}]
\end{equation}
\begin{equation}
    b=\frac{(B_r+B_w)/10^9}{t}=\frac{(2 \cdot B_{r/w})/10^9}{t}=\frac{(2 \cdot 4 \cdot 4096^2)/10^9}{t}\qquad[\textnormal{GB/s}]
\end{equation}

\begin{figure}[h!tb]
    \centering
    \captionsetup{
        type=plot,
        belowskip=10pt,
    }
    \caption{Bandwidth by block size}
    \begin{tikzpicture}
        \begin{axis}[
            width=0.8\textwidth,
            height=0.5\textwidth,
            xlabel={Block Size (2\symbol{"005E}Exp)},
            ylabel={Bandwidth (GB/s)},
            legend pos=north west,
            grid=major,
        ]

        \addplot table [x=Exp, y=Average] {
            Exp Average
            2	1.008
            3	0.980
            4	0.936
            5	0.966
            6	1.043
            7	0.953
            8	1.052
        };

        \addplot table [x=Exp, y=Average] {
            Exp Average
            2	5.953
            3	7.690
            4	6.986
            5	9.229
            6	10.628
            7	10.568
            8	11.006
        };
        \legend{Sequential, Parallel}

        \end{axis}
    \end{tikzpicture}
\end{figure}

\end{document}